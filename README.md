# kenya_gazette

Text Data Analysis of kenya gazette

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
|:------:|:------:|:-------|:-------|:-------|:------:|:------:|:-------|:-------|:-------|
| [1890](./pdf_file/1890.pdf) | [1891](./pdf_file/1891.pdf) | [1892](./pdf_file/1892.pdf) | [1893](./pdf_file/1893.pdf) | [1894](./pdf_file/1894.pdf) | [1895](./pdf_file/1895.pdf) | [1896](./pdf_file/1896.pdf) | [1897](./pdf_file/1897.pdf) | [1898](./pdf_file/1898.pdf) | [1899](./pdf_file/1899.pdf) |
| [1900](./pdf_file/1900.pdf) | [1901](./pdf_file/1901.pdf) | [1902](./pdf_file/1902.pdf) | [1903](./pdf_file/1903.pdf) | [1904](./pdf_file/1904.pdf) | [1905](./pdf_file/1905.pdf) | [1906](./pdf_file/1906.pdf) | [1907](./pdf_file/1907.pdf) | [1908](./pdf_file/1908.pdf) | [1909](./pdf_file/1909.pdf) |
| [1910](./pdf_file/1910.pdf) | [1911](./pdf_file/1911.pdf) | [1912](./pdf_file/1912.pdf) | [1913](./pdf_file/1913.pdf) | [1914](./pdf_file/1914.pdf) | [1915](./pdf_file/1915.pdf) | [1916](./pdf_file/1916.pdf) | [1917](./pdf_file/1917.pdf) | [1918](./pdf_file/1918.pdf) | [1919](./pdf_file/1919.pdf) |
| [1920](./pdf_file/1920.pdf) | [1921](./pdf_file/1921.pdf) | [1922](./pdf_file/1922.pdf) | [1923](./pdf_file/1923.pdf) | [1924](./pdf_file/1924.pdf) | [1925](./pdf_file/1925.pdf) | [1926](./pdf_file/1926.pdf) | [1927](./pdf_file/1927.pdf) | [1928](./pdf_file/1928.pdf) | [1929](./pdf_file/1929.pdf) |
| [1930](./pdf_file/1930.pdf) | [1931](./pdf_file/1931.pdf) | [1932](./pdf_file/1932.pdf) | [1933](./pdf_file/1933.pdf) | [1934](./pdf_file/1934.pdf) | [1935](./pdf_file/1935.pdf) | [1936](./pdf_file/1936.pdf) | [1937](./pdf_file/1937.pdf) | [1938](./pdf_file/1938.pdf) | [1939](./pdf_file/1939.pdf) |
| [1940](./pdf_file/1940.pdf) | [1941](./pdf_file/1941.pdf) | [1942](./pdf_file/1942.pdf) | [1943](./pdf_file/1943.pdf) | [1944](./pdf_file/1944.pdf) | [1945](./pdf_file/1945.pdf) | [1946](./pdf_file/1946.pdf) | [1947](./pdf_file/1947.pdf) | [1948](./pdf_file/1948.pdf) | [1949](./pdf_file/1949.pdf) |
| [1950](./pdf_file/1950.pdf) | [1951](./pdf_file/1951.pdf) | [1952](./pdf_file/1952.pdf) | [1953](./pdf_file/1953.pdf) | [1954](./pdf_file/1954.pdf) | [1955](./pdf_file/1955.pdf) | [1956](./pdf_file/1956.pdf) | [1957](./pdf_file/1957.pdf) | [1958](./pdf_file/1958.pdf) | [1959](./pdf_file/1959.pdf) |
| [1960](./pdf_file/1960.pdf) | [1961](./pdf_file/1961.pdf) | [1962](./pdf_file/1962.pdf) | [1963](./pdf_file/1963.pdf) | [1964](./pdf_file/1964.pdf) | [1965](./pdf_file/1965.pdf) | [1966](./pdf_file/1966.pdf) | [1967](./pdf_file/1967.pdf) | [1968](./pdf_file/1968.pdf) | [1969](./pdf_file/1969.pdf) |
| [1970](./pdf_file/1970.pdf) | [1971](./pdf_file/1971.pdf) | [1972](./pdf_file/1972.pdf) | [1973](./pdf_file/1973.pdf) | [1974](./pdf_file/1974.pdf) | [1975](./pdf_file/1975.pdf) | [1976](./pdf_file/1976.pdf) | [1977](./pdf_file/1977.pdf) | [1978](./pdf_file/1978.pdf) | [1979](./pdf_file/1979.pdf) |
| [1980](./pdf_file/1980.pdf) | [1981](./pdf_file/1981.pdf) | [1982](./pdf_file/1982.pdf) | [1983](./pdf_file/1983.pdf) | [1984](./pdf_file/1984.pdf) | [1985](./pdf_file/1985.pdf) | [1986](./pdf_file/1986.pdf) | [1987](./pdf_file/1987.pdf) | [1988](./pdf_file/1988.pdf) | [1989](./pdf_file/1989.pdf) |
| [1990](./pdf_file/1990.pdf) | [1991](./pdf_file/1991.pdf) | [1992](./pdf_file/1992.pdf) | [1993](./pdf_file/1993.pdf) | [1994](./pdf_file/1994.pdf) | [1995](./pdf_file/1995.pdf) | [1996](./pdf_file/1996.pdf) | [1997](./pdf_file/1997.pdf) | [1998](./pdf_file/1998.pdf) | [1999](./pdf_file/1999.pdf) |
| [2000](./pdf_file/2000.pdf) | [2001](./pdf_file/2001.pdf) | [2002](./pdf_file/2002.pdf) | [2003](./pdf_file/2003.pdf) | [2004](./pdf_file/2004.pdf) | [2005](./pdf_file/2005.pdf) | [2006](./pdf_file/2006.pdf) | [2007](./pdf_file/2007.pdf) | [2008](./pdf_file/2008.pdf) | [2009](./pdf_file/2009.pdf) |
| [2010](./pdf_file/2010.pdf) | [2011](./pdf_file/2011.pdf) | [2012](./pdf_file/2012.pdf) | [2013](./pdf_file/2013.pdf) | [2014](./pdf_file/2014.pdf) | [2015](./pdf_file/2015.pdf) | [2016](./pdf_file/2016.pdf) | [2017](./pdf_file/2017.pdf) | [2018](./pdf_file/2018.pdf) | [2019](./pdf_file/2019.pdf) |
| [2020](./pdf_file/2020.pdf) | [2021](./pdf_file/2021.pdf) | [2022](./pdf_file/2022.pdf) | [2023](./pdf_file/2023.pdf) | [2024](./pdf_file/2024.pdf) | [2025](./pdf_file/2025.pdf) | [2026](./pdf_file/2026.pdf) | [2027](./pdf_file/2027.pdf) | [2028](./pdf_file/2028.pdf) | [2029](./pdf_file/2029.pdf) |

## Notable Mentions

General Elections Kenya Analysis and Shiny Dashboards

-   <https://akhapwoyachris.shinyapps.io/presidentialelectionkenya2022/>
-   <https://christopherakhapwoya.shinyapps.io/electionkenya/>
-   [Fiver Services](https://www.fiverr.com/s/jj5dYam)

## [Data Source Scripts](./R/updated_get_individual_links.R)

R File: ./R/updated_get_individual_links.R Steps to download the files

-   get links from tables
-   download all files if they done exist on local
-   if on second run they exist, compare online size and download file and if not equal redownloded
-   second else ensures all files are downloaded irrespective of size comparison

```{r}
# set saving of raw data
date_append = gsub(pattern = "[[:punct:]]", replacement = '_', x = Sys.Date())
# date_append
dir.create(
  paste('./data-raw',
        date_append, sep = '/'
  ), recursive = T
)
#
# importing packages 
library(httr) 
library(XML) 
library(tidyverse)
library(rvest)

# get all links of parent links ie yearly links
url <- "https://new.kenyalaw.org/gazettes/"
file_Read <- read_html(url)
links_tables_1 <- file_Read |> 
  html_nodes(xpath = '//*[@id="top"]/div[2]/div[2]') |> 
  #html_elements('h2') |> 
  html_elements('a') |> 
  html_attr('href') |> 
  str_subset(pattern = "#", negate = T) |>
  paste0('https://new.kenyalaw.org', . = _, sep = '')
#
links_tables_1
#
# get links to Gazette pdf files
links_df = data.frame()
length_year_df = length(links_tables_1)
for (i in 1:length_year_df){
  cat(i, length_year_df, '\n')
  url2 = links_tables_1[i]
  link2_Read <- read_html(url2)
  # it is a table to we scrap table, get the underlying hrefs too
  link2_Read_node = link2_Read |>
    html_nodes(xpath = '//*[@id="doc-table"]')
  table = link2_Read_node |>
    html_table() |>
    (\(x) x[[1]])() |> 
    janitor::clean_names() |> 
    filter(grepl(pattern = "gazette", title, ignore.case = T)) |>
    mutate(across(everything(), .fns = as.character))
  link = link2_Read_node |> 
    html_elements('a') |>
    html_attr('href') |>
    paste0('https://new.kenyalaw.org', . = _, sep = '')
  #
  links_df = bind_rows(
    links_df, 
    cbind(table, data.frame(links = link)))
}
#
View(links_df)

# create path and place to file to save
links_df_2 = links_df |>
  mutate(
    links2 = paste(links, 'source', sep = '/'),
    path = paste(
      "pdf_files",
      format(lubridate::dmy(date), "%Y/%b"),
      paste(
        str_replace_all(
          string = title,
          c("[[:punct:]]" = '', " " = "_")), '.pdf', sep = ''),
      sep = '/')
  )
#
# create path function to enable directory creation during download
make_path <- function(path){
  dir.create(dirname(path = path), recursive = TRUE, showWarnings = F);
  path
}
# download files
length_files = length(links_df_2$links2)
incompletes_files = data.frame() # saves files with problems
for (url_index in 1:length_files) {
  print(url_index)
  url_str <- links_df_2$links2[url_index]
  dest <- links_df_2$path[url_index]
  dest = make_path(dest)
  #
  # download pdf given link and save in file 
  if (!file.exists(dest)) {
    tryCatch(
      download.file(
        url = url_str, 
        destfile = dest, 
        quiet = F, mode = 'wb'), 
      error = function(e) {print('broken')} 
    )
  } else {  # comment out this else on first run
    # use the else if file exists to check if file online is different from local
    # if not download again
    expected_file_size <- httr::HEAD(url_str)$headers$`content-length`
    dest_files_size = file.size(dest)
    if (expected_file_size != dest_files_size){
      incompletes_files = bind_rows(
        incompletes_files,
        bind_cols(
          links_df[url_index,], data.frame(destination = dest)
          )
      )
      print(paste("INCOMPLETE: Downloading again", dest, sep = " "))
      #
      tryCatch(
        download.file(
          url = url_str,
          destfile = dest,
          quiet = F, mode = 'wb'),
        error = function(e) {print('broken')}
      )

    } 
    # else { # if file exists locally skip # use only one else at a time
    #  print(url_index)
    #  next
    # }
  }
}
# save worksapce
date_append = gsub(pattern = "[[:punct:]]", replacement = '_', x = Sys.Date())
date_append
save.image(
  paste('./workspaces/',
        date_append, '.RData',sep = ''
  )
)
```

## [Conversion to text and jpeg for analysis](./R/to_png.R)

```{r}
library(pdftools)
library(magick)
library(tesseract)
#
dir.create('text_files')
dir.create('jpeg_files')
dir.create('text-raw')
#
list_of_files <- list.files("pdf_files/", recursive = T, full.names = T)
len_list_of_files = length(list_of_files)
len_list_of_files
# make path
make_path <- function(path){
  dir.create(dirname(path = path), recursive = TRUE, showWarnings = F);
  path
}
# start conversions
for (pdf_file_index in 1:len_list_of_files) {
  percentum = 100*pdf_file_index/len_list_of_files
  print(percentum)
  pdf_file = list_of_files[pdf_file_index]
  pages_in = pdf_info(pdf = pdf_file)$pages
  pdf_file_text <- tryCatch(
    pdf_text(pdf = pdf_file), 
    error = function(e) {
      write.table(pdf_file, 'broken_links.txt', append = T)
    }
  )
  #
  # jpeg
  png_file_path = gsub(
    x = pdf_file, pattern = "pdf_files/|.pdf", replacement = ""
    ) |> paste('jpeg_files/',sep = '', .=_) |> 
    paste('/',sep = '')#paste('.jpeg', sep = '')
  #
  for (i in 1:pages_in){
    png_file_p = make_path(
      png_file_path |> 
        paste('page', i, '.png', sep = '')
      ) 
    png::writePNG(
      target = png_file_p, 
      pdf_render_page(
        pdf = pdf_file, 
        page = i, dpi = 250), metadata = T)
  }
  # txt file
  text_file_path = gsub(
    x = pdf_file, pattern = "pdf_files/|.pdf", replacement = ""
  ) |> paste('text_files/',sep = '', .=_)
  text_file_name = make_path(text_file_path)  |> 
    paste('.txt', sep = '')
  #
  # all_text
  pdf_file_text <- pdf_text(pdf = pdf_file)
  write.table(pdf_file_text, text_file_name, append = F, col.names = NA)
  # master file
  # write.table(pdf_file_text, 'text-raw/pdf_file_text2.txt', append = T, col.names = NA)
}
#
```

## The Contents table generation

```{python}
def print_decades(start_year, end_year):
  """
  Prints a table of years by decade from start_year to end_year with "|" as delimiter 
  and each year as a Markdown link to a corresponding PDF file.

  Args:
    start_year: The starting year.
    end_year: The ending year.
  """
  for decade_start in range(start_year, end_year + 1, 10):
    decade_end = min(decade_start + 9, end_year) 
    row_str = "|"
    for year in range(decade_start, decade_end + 1):
      row_str += f"[{year}](./pdf_file/{year}.pdf) | " 
    print(row_str) 

# Print the table of decades
print_decades(1890, 2029)
```
