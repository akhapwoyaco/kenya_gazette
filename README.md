# kenya_gazette

Text Data Analysis of kenya gazette

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
|:------:|:------:|:-------|:-------|:-------|:------:|:------:|:-------|:-------|:-------|
|[1890](./pdf_file/1890) | [1891](./pdf_file/1891) | [1892](./pdf_file/1892) | [1893](./pdf_file/1893) | [1894](./pdf_file/1894) | [1895](./pdf_file/1895) | [1896](./pdf_file/1896) | [1897](./pdf_file/1897) | [1898](./pdf_file/1898) | [1899](./pdf_file/1899) | 
|[1900](./pdf_file/1900) | [1901](./pdf_file/1901) | [1902](./pdf_file/1902) | [1903](./pdf_file/1903) | [1904](./pdf_file/1904) | [1905](./pdf_file/1905) | [1906](./pdf_file/1906) | [1907](./pdf_file/1907) | [1908](./pdf_file/1908) | [1909](./pdf_file/1909) | 
|[1910](./pdf_file/1910) | [1911](./pdf_file/1911) | [1912](./pdf_file/1912) | [1913](./pdf_file/1913) | [1914](./pdf_file/1914) | [1915](./pdf_file/1915) | [1916](./pdf_file/1916) | [1917](./pdf_file/1917) | [1918](./pdf_file/1918) | [1919](./pdf_file/1919) | 
|[1920](./pdf_file/1920) | [1921](./pdf_file/1921) | [1922](./pdf_file/1922) | [1923](./pdf_file/1923) | [1924](./pdf_file/1924) | [1925](./pdf_file/1925) | [1926](./pdf_file/1926) | [1927](./pdf_file/1927) | [1928](./pdf_file/1928) | [1929](./pdf_file/1929) | 
|[1930](./pdf_file/1930) | [1931](./pdf_file/1931) | [1932](./pdf_file/1932) | [1933](./pdf_file/1933) | [1934](./pdf_file/1934) | [1935](./pdf_file/1935) | [1936](./pdf_file/1936) | [1937](./pdf_file/1937) | [1938](./pdf_file/1938) | [1939](./pdf_file/1939) | 
|[1940](./pdf_file/1940) | [1941](./pdf_file/1941) | [1942](./pdf_file/1942) | [1943](./pdf_file/1943) | [1944](./pdf_file/1944) | [1945](./pdf_file/1945) | [1946](./pdf_file/1946) | [1947](./pdf_file/1947) | [1948](./pdf_file/1948) | [1949](./pdf_file/1949) | 
|[1950](./pdf_file/1950) | [1951](./pdf_file/1951) | [1952](./pdf_file/1952) | [1953](./pdf_file/1953) | [1954](./pdf_file/1954) | [1955](./pdf_file/1955) | [1956](./pdf_file/1956) | [1957](./pdf_file/1957) | [1958](./pdf_file/1958) | [1959](./pdf_file/1959) | 
|[1960](./pdf_file/1960) | [1961](./pdf_file/1961) | [1962](./pdf_file/1962) | [1963](./pdf_file/1963) | [1964](./pdf_file/1964) | [1965](./pdf_file/1965) | [1966](./pdf_file/1966) | [1967](./pdf_file/1967) | [1968](./pdf_file/1968) | [1969](./pdf_file/1969) | 
|[1970](./pdf_file/1970) | [1971](./pdf_file/1971) | [1972](./pdf_file/1972) | [1973](./pdf_file/1973) | [1974](./pdf_file/1974) | [1975](./pdf_file/1975) | [1976](./pdf_file/1976) | [1977](./pdf_file/1977) | [1978](./pdf_file/1978) | [1979](./pdf_file/1979) | 
|[1980](./pdf_file/1980) | [1981](./pdf_file/1981) | [1982](./pdf_file/1982) | [1983](./pdf_file/1983) | [1984](./pdf_file/1984) | [1985](./pdf_file/1985) | [1986](./pdf_file/1986) | [1987](./pdf_file/1987) | [1988](./pdf_file/1988) | [1989](./pdf_file/1989) | 
|[1990](./pdf_file/1990) | [1991](./pdf_file/1991) | [1992](./pdf_file/1992) | [1993](./pdf_file/1993) | [1994](./pdf_file/1994) | [1995](./pdf_file/1995) | [1996](./pdf_file/1996) | [1997](./pdf_file/1997) | [1998](./pdf_file/1998) | [1999](./pdf_file/1999) | 
|[2000](./pdf_file/2000) | [2001](./pdf_file/2001) | [2002](./pdf_file/2002) | [2003](./pdf_file/2003) | [2004](./pdf_file/2004) | [2005](./pdf_file/2005) | [2006](./pdf_file/2006) | [2007](./pdf_file/2007) | [2008](./pdf_file/2008) | [2009](./pdf_file/2009) | 
|[2010](./pdf_file/2010) | [2011](./pdf_file/2011) | [2012](./pdf_file/2012) | [2013](./pdf_file/2013) | [2014](./pdf_file/2014) | [2015](./pdf_file/2015) | [2016](./pdf_file/2016) | [2017](./pdf_file/2017) | [2018](./pdf_file/2018) | [2019](./pdf_file/2019) | 
|[2020](./pdf_file/2020) | [2021](./pdf_file/2021) | [2022](./pdf_file/2022) | [2023](./pdf_file/2023) | [2024](./pdf_file/2024) | [2025](./pdf_file/2025) | [2026](./pdf_file/2026) | [2027](./pdf_file/2027) | [2028](./pdf_file/2028) | [2029](./pdf_file/2029) | 
## Notable Mentions

General Elections Kenya Analysis and Shiny Dashboards

-   <https://akhapwoyachris.shinyapps.io/presidentialelectionkenya2022/>
-   <https://christopherakhapwoya.shinyapps.io/electionkenya/>
-   [Fiver Services](https://www.fiverr.com/s/jj5dYam)

## [Data Source Scripts](./R/updated_get_individual_links.R)

R File: ./R/updated_get_individual_links.R Steps to download the files

-   get links from tables
-   download all files if they done exist on local
-   if on second run they exist, compare online size and download file and if not equal redownloded
-   second else ensures all files are downloaded irrespective of size comparison

```{r}
# set saving of raw data
date_append = gsub(pattern = "[[:punct:]]", replacement = '_', x = Sys.Date())
# date_append
dir.create(
  paste('./data-raw',
        date_append, sep = '/'
  ), recursive = T
)
#
# importing packages 
library(httr) 
library(XML) 
library(tidyverse)
library(rvest)

# get all links of parent links ie yearly links
url <- "https://new.kenyalaw.org/gazettes/"
file_Read <- read_html(url)
links_tables_1 <- file_Read |> 
  html_nodes(xpath = '//*[@id="top"]/div[2]/div[2]') |> 
  #html_elements('h2') |> 
  html_elements('a') |> 
  html_attr('href') |> 
  str_subset(pattern = "#", negate = T) |>
  paste0('https://new.kenyalaw.org', . = _, sep = '')
#
links_tables_1
#
# get links to Gazette pdf files
links_df = data.frame()
length_year_df = length(links_tables_1)
for (i in 1:length_year_df){
  cat(i, length_year_df, '\n')
  url2 = links_tables_1[i]
  link2_Read <- read_html(url2)
  # it is a table to we scrap table, get the underlying hrefs too
  link2_Read_node = link2_Read |>
    html_nodes(xpath = '//*[@id="doc-table"]')
  table = link2_Read_node |>
    html_table() |>
    (\(x) x[[1]])() |> 
    janitor::clean_names() |> 
    filter(grepl(pattern = "gazette", title, ignore.case = T)) |>
    mutate(across(everything(), .fns = as.character))
  link = link2_Read_node |> 
    html_elements('a') |>
    html_attr('href') |>
    paste0('https://new.kenyalaw.org', . = _, sep = '')
  #
  links_df = bind_rows(
    links_df, 
    cbind(table, data.frame(links = link)))
}
#
View(links_df)

# create path and place to file to save
links_df_2 = links_df |>
  mutate(
    links2 = paste(links, 'source', sep = '/'),
    path = paste(
      "pdf_files",
      format(lubridate::dmy(date), "%Y/%b"),
      paste(
        str_replace_all(
          string = title,
          c("[[:punct:]]" = '', " " = "_")), '.pdf', sep = ''),
      sep = '/')
  )
#
# create path function to enable directory creation during download
make_path <- function(path){
  dir.create(dirname(path = path), recursive = TRUE, showWarnings = F);
  path
}
# download files
length_files = length(links_df_2$links2)
incompletes_files = data.frame() # saves files with problems
for (url_index in 1:length_files) {
  print(url_index)
  url_str <- links_df_2$links2[url_index]
  dest <- links_df_2$path[url_index]
  dest = make_path(dest)
  #
  # download pdf given link and save in file 
  if (!file.exists(dest)) {
    tryCatch(
      download.file(
        url = url_str, 
        destfile = dest, 
        quiet = F, mode = 'wb'), 
      error = function(e) {print('broken')} 
    )
  } else {  # comment out this else on first run
    # use the else if file exists to check if file online is different from local
    # if not download again
    expected_file_size <- httr::HEAD(url_str)$headers$`content-length`
    dest_files_size = file.size(dest)
    if (expected_file_size != dest_files_size){
      incompletes_files = bind_rows(
        incompletes_files,
        bind_cols(
          links_df[url_index,], data.frame(destination = dest)
          )
      )
      print(paste("INCOMPLETE: Downloading again", dest, sep = " "))
      #
      tryCatch(
        download.file(
          url = url_str,
          destfile = dest,
          quiet = F, mode = 'wb'),
        error = function(e) {print('broken')}
      )

    } 
    # else { # if file exists locally skip # use only one else at a time
    #  print(url_index)
    #  next
    # }
  }
}
# save worksapce
date_append = gsub(pattern = "[[:punct:]]", replacement = '_', x = Sys.Date())
date_append
save.image(
  paste('./workspaces/',
        date_append, '.RData',sep = ''
  )
)
```

## [Conversion to text and jpeg for analysis](./R/to_png.R)

```{r}
library(pdftools)
library(magick)
library(tesseract)
#
dir.create('text_files')
dir.create('jpeg_files')
dir.create('text-raw')
#
list_of_files <- list.files("pdf_files/", recursive = T, full.names = T)
len_list_of_files = length(list_of_files)
len_list_of_files
# make path
make_path <- function(path){
  dir.create(dirname(path = path), recursive = TRUE, showWarnings = F);
  path
}
# start conversions
for (pdf_file_index in 1:len_list_of_files) {
  percentum = 100*pdf_file_index/len_list_of_files
  print(percentum)
  pdf_file = list_of_files[pdf_file_index]
  pages_in = pdf_info(pdf = pdf_file)$pages
  pdf_file_text <- tryCatch(
    pdf_text(pdf = pdf_file), 
    error = function(e) {
      write.table(pdf_file, 'broken_links.txt', append = T)
    }
  )
  #
  # jpeg
  png_file_path = gsub(
    x = pdf_file, pattern = "pdf_files/|.pdf", replacement = ""
    ) |> paste('jpeg_files/',sep = '', .=_) |> 
    paste('/',sep = '')#paste('.jpeg', sep = '')
  #
  for (i in 1:pages_in){
    png_file_p = make_path(
      png_file_path |> 
        paste('page', i, '.png', sep = '')
      ) 
    png::writePNG(
      target = png_file_p, 
      pdf_render_page(
        pdf = pdf_file, 
        page = i, dpi = 250), metadata = T)
  }
  # txt file
  text_file_path = gsub(
    x = pdf_file, pattern = "pdf_files/|.pdf", replacement = ""
  ) |> paste('text_files/',sep = '', .=_)
  text_file_name = make_path(text_file_path)  |> 
    paste('.txt', sep = '')
  #
  # all_text
  pdf_file_text <- pdf_text(pdf = pdf_file)
  write.table(pdf_file_text, text_file_name, append = F, col.names = NA)
  # master file
  # write.table(pdf_file_text, 'text-raw/pdf_file_text2.txt', append = T, col.names = NA)
}
#
```

## The Contents table generation

```{python}
def print_decades(start_year, end_year):
  """
  Prints a table of years by decade from start_year to end_year with "|" as delimiter 
  and each year as a Markdown link to a corresponding PDF file.

  Args:
    start_year: The starting year.
    end_year: The ending year.
  """
  for decade_start in range(start_year, end_year + 1, 10):
    decade_end = min(decade_start + 9, end_year) 
    row_str = "|"
    for year in range(decade_start, decade_end + 1):
      row_str += f"[{year}](./pdf_file/{year}) | " 
    print(row_str) 

# Print the table of decades
print_decades(1890, 2029)
```
